{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPR and Lidar\n",
    "\n",
    "This notebook is designed to obtain snow depth, density, and SWE from ground-penetrating radar data. We will use GPR data gathered during the SnowEx Alaska campaigns, with validation from the airborne lidar obtained during the same campaign.\n",
    "\n",
    "The text and code in this notebook is adapted from Randall Bonnell's tutorial on lidar and GPR during the 2024 SnowEx Hackweek, found here: https://snowex-2024.hackweek.io/tutorials/gpr_lidar/GPR_Lidar_HackweekTutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U snowexsql"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Title Card](./images/gpr_lidar/HackweekGraphicalAbstract_GPR_Lidar_v2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. GPR Methods for the Retrieval of Snow Depth and SWE\n",
    "\n",
    "## What is GPR?\n",
    "* GPR transmits a radar signal into the snowpack, which then reflects off objects/interfaces with contrasting dielectric permittivity. The GPR records the **amplitude** and **two-way travel time (twt)** of the reflections.\n",
    "* **Dielectric permittivity** refers to the dielectric properties of the snowpack that define how EM energy trasmits through the medium.\n",
    "* Usually, we are interested in the snow-ground interface, and we measure the snowpack thickness (depth) in two-way travel time (in nanoseconds).\n",
    "* Most analysis-ready GPR products have twt, snow depth, and SWE variables. Some have also been updated to include derived snow density\n",
    "\n",
    "For this notebook, we will start from the two-way travel time data to derive our snow properties of interest, and compare it to airborne lidar data. The `snowexsql` package will be needed.\n",
    "\n",
    "Note that the results derived here are for the user's reference - most of the GPR products posted on NSIDC already have snow depth and SWE available as variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SnowEx23 GPR/Lidar Derived Permittivities/Densities in the Boreal Forest, Alaska\n",
    "\n",
    "## Deriving Snow Density at Farmer's Loop/Creamer's Field\n",
    "For this first step, we will use:\n",
    "1. Airborne lidar data collected on 11 March, 2023.\n",
    "2. GPR data collected on 7, 11, 13, and 16 March, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 Load relevant packages\n",
    "import os\n",
    "import numpy as np \n",
    "from datetime import date\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "#packages for figures\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "\n",
    "#geospatial packages\n",
    "import geopandas as gpd #for vector data\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from shapely.geometry import box, Point\n",
    "\n",
    "import rasterio as rio\n",
    "\n",
    "from snowexsql.lambda_client import SnowExLambdaClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "client = SnowExLambdaClient()\n",
    "\n",
    "# Get all measurement classes dynamically\n",
    "classes = client.get_measurement_classes()\n",
    "LayerMeasurements = classes['LayerMeasurements']\n",
    "PointMeasurements = classes['PointMeasurements']\n",
    "RasterMeasurements = classes['RasterMeasurements']\n",
    "\n",
    "print(\"ðŸ” Testing Lambda connection...\")\n",
    "connection_test = client.test_connection()\n",
    "print(f\"âœ… Connected: {connection_test.get('connected', False)}\")\n",
    "if connection_test.get('connected'):\n",
    "    print(f\"ðŸ“Š Database: {connection_test.get('version', 'Unknown version')}\")\n",
    "else:\n",
    "    print(\"âŒ Connection failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Load the GPR data from the SnowEx data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query all point measurements within 1000m of Creamer's Field center\n",
    "# Creamer's Field coordinates: 64.864167Â°N, 147.737778Â°W (EPSG:4326)\n",
    "# Converting to a Point geometry for the query\n",
    "\n",
    "# Center point of Creamer's Field\n",
    "creamers_field_center = Point(-147.737778, 64.864167)\n",
    "\n",
    "# Query with 1000m buffer around the center point\n",
    "df = PointMeasurements.from_area(\n",
    "    pt=creamers_field_center,\n",
    "    crs=4326,  # WGS84 coordinate system\n",
    "    buffer=1000,  # 1000 meters\n",
    "    type='two_way_travel',\n",
    "    observer='Randall Bonnell',\n",
    "    limit=20000,  # Adjust limit as needed\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of gpr two-way travel times and estimated snow depths\n",
    "#Estimate snow depths from twt by assuming a velocity of 0.25 m/ns --> Is this an appropriate velocity estimate?\n",
    "df['Depth_estimated'] = (df['value']/2)*0.25\n",
    "\n",
    "ax1 = df.plot.hist(column=[\"value\"], edgecolor='black', title='two-way travel time (ns)')\n",
    "ax2 = df.plot.hist(column=[\"Depth_estimated\"], edgecolor='black', title='Snow depth (m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Lidar-Derived Canopy Height and Snow Depth\n",
    "To compare against the GPR data, we will load airborne lidar data products, specifically canopy height and snow depths. To facilitate our analysis, we will create a bounding box for the lidar results using our GPR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x/y limit from GPR data\n",
    "bounds = df.total_bounds\n",
    "\n",
    "# Create a bounding box\n",
    "gpr_limits = box(*bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from this bounding box, accessing the lidar data is very similar to the GPR data. However, because the lidar data is much larger, loading it through the database may take a few minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commented until we fix reading of raster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lidar canopy heights\n",
    "#flcf_ch = RasterMeasurements.from_area(shp=gpr_limits,\n",
    "#                                      crs=26906,\n",
    "#                                      buffer=None,\n",
    "#                                      type='canopy_height',\n",
    "#                                      site_name='farmers-creamers',\n",
    "#                                      observers='chris larsen'\n",
    "#                                      )\n",
    "#print(flcf_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lidar snow depths\n",
    "#flcf_sd = RasterMeasurements.from_area(shp=gpr_limits,\n",
    "#                                       crs=26906,\n",
    "#                                       buffer=None,\n",
    "#                                       type='depth',\n",
    "#                                       site_name='farmers-creamers',\n",
    "#                                       observers='chris larsen'\n",
    "#                                      )\n",
    "#print(flcf_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match GPR data with lidar data\n",
    "To match the GPR and lidar data, we can either rasterize the GPR data or vectorize the lidar data. For simplicity, we will vectorize the lidar data and perform a nearest neighbor search.\n",
    "\n",
    "The GPR data is ~0.1 m resolution, whereas the lidar data is ~0.5 m resolution. So, we can expect ~5 GPR data points per lidar pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find height and width of array\n",
    "# height, width = flcf_sd.read(1).shape\n",
    "\n",
    "# # Create arrays of the height, width of the lidar raster\n",
    "# cols, rows = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "# # Get the easting/northing from the lidar raster\n",
    "# x_lidar, y_lidar = rio.transform.xy(flcf_sd.transform, rows, cols)\n",
    "\n",
    "# # Vectorize the raster data\n",
    "# x_lidar_vec = np.array(x_lidar).flatten()\n",
    "# y_lidar_vec = np.array(y_lidar).flatten()\n",
    "# flcf_sd_vec = flcf_sd.read().flatten()\n",
    "\n",
    "# # Pull vectors from GPR GeoDataFrame\n",
    "# gpr_arr = np.stack([gpr_df.geometry.x, gpr_df.geometry.y, gpr_df['value']], axis=1)\n",
    "# gpr_x = gpr_arr[:,0]\n",
    "# gpr_y = gpr_arr[:,1]\n",
    "# gpr_twt = gpr_arr[:,2].reshape(len(gpr_arr[:,2]), 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the nearest-neighbor approach, we will be using a K-D tree to efficiently find adjacent points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create coordinate sets for nearest neighbor search\n",
    "# set1 = np.column_stack((x_lidar_vec, y_lidar_vec))\n",
    "# set2 = np.column_stack((gpr_x, gpr_y))\n",
    "\n",
    "# # Build KDTree from GPR coordinates\n",
    "# tree = cKDTree(set2)\n",
    "\n",
    "# # Define the search radius (meters)\n",
    "# radius = 0.25\n",
    "\n",
    "# # Create function to find median travel times\n",
    "# def median_travel_time(point, gpr_kdtree, gpr_coordinates, gpr_twt, radius):\n",
    "#     indices = tree.query_ball_point(point, radius)\n",
    "#     if indices:\n",
    "#         # Retrieve travel times for the nearest neighbors\n",
    "#         neighbor_twt = gpr_twt[indices]\n",
    "#         median_twt = np.median(neighbor_twt)\n",
    "\n",
    "#         return median_twt\n",
    "\n",
    "#     else:\n",
    "#         return np.nan\n",
    "\n",
    "# # Find twt medians for each lidar point\n",
    "# twt_median = np.array([median_travel_time(point,tree,set2,gpr_twt,radius) for point in set1])\n",
    "# print(twt_median)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPR data is not as spatially continuous as the lidar data, so `twt_median` has a lot of NaN values. To reduce memory usage, let's remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create mask for GPR NaN values\n",
    "# mask = np.isnan(twt_median)\n",
    "\n",
    "# # Mask GPR/lidar vectors\n",
    "# flcf_sd_vec_clean = flcf_sd_vec[~mask]\n",
    "# set1_clean = set1[~mask]\n",
    "# twt_median_clean = twt_median[~mask]\n",
    "\n",
    "# print(twt_median_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is matched, and we have removed NaN values, we can now calculate relative permittivity and snow density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate relative permittivity\n",
    "# c = 0.2998 # Speed of light in a vaccuum\n",
    "# e_s = ((c*twt_median_clean) / (2*flcf_sd_vec_clean))**2\n",
    "\n",
    "# # Calculate snow density\n",
    "# rho_s = ((np.sqrt(e_s)-1) / 0.845)*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the derived densities\n",
    "Now that we've gone through all of this trouble, let's take a look at the derived snow densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot snow density\n",
    "# plt.figure()\n",
    "# plt.scatter(set1_clean[:,0], set1_clean[:,1], s=10,\n",
    "#             c=rho_s, cmap='viridis', clim=(0,500),\n",
    "#             edgecolor=None\n",
    "#            )\n",
    "# plt.colorbar()\n",
    "# plt.title('Snow density [kg m-3]')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the histogram distribution of snow density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define bin edges\n",
    "# bin_edges = np.arange(np.min(rho_s), np.max(rho_s), 25)\n",
    "\n",
    "# # Create the histogram\n",
    "# plt.figure()\n",
    "# plt.hist(rho_s, bins=bin_edges, edgecolor=None)\n",
    "# plt.xlabel('Snow density [kg m-3]')\n",
    "# plt.ylabel('Counts')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some pretty unrealistic values in the data, so let's zoom in to more realistic densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-define bin edges\n",
    "# bin_edges = np.arange(0, 500, 25)\n",
    "\n",
    "# # Create refined histogram\n",
    "# plt.figure()\n",
    "# plt.hist(rho_s, bins=bin_edges, edgecolor='black')\n",
    "# plt.xlabel('Snow density [kg m-3]')\n",
    "# plt.ylabel('Counts')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even at this range, there is a fair amount of random error. This may be caused by the measurement accuracy of the lidar or the GPR, the depth of the snow, and/or geolocation uncertainty. \n",
    "\n",
    "A user could further improve these densities by upsampling the lidar to match the GPR's geolocation uncertainty (3 m in this case), remove erroneous values in the snow depth or permittivity data, or run a spatial averaging filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
